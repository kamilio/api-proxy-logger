# fly.toml app configuration file generated for llm_debugger
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.

app = "llm_debugger"
primary_region = "iad"  # Change to your preferred region (e.g., "lhr" for London, "fra" for Frankfurt)

[build]
  # Use Dockerfile for building
  dockerfile = "Dockerfile"

[env]
  # Port configuration
  PORT = "8080"
  NODE_ENV = "production"

  # Paths configuration - these can be overridden
  CONFIG_PATH = "/app/config/config.yaml"
  LOG_OUTPUT_DIR = "/app/logs"
  RESPONSES_PATH = "/app/logs/responses.yaml"

  # Optional: Set a default proxy port if different from 8080
  # PROXY_PORT = "8080"

[[services]]
  protocol = "tcp"
  internal_port = 8080
  processes = ["app"]

  [[services.ports]]
    port = 80
    handlers = ["http"]
    force_https = true

  [[services.ports]]
    port = 443
    handlers = ["tls", "http"]

  [services.concurrency]
    type = "connections"
    hard_limit = 25
    soft_limit = 20

  [[services.tcp_checks]]
    interval = "15s"
    timeout = "2s"
    grace_period = "1s"

  [[services.http_checks]]
    interval = "30s"
    grace_period = "5s"
    method = "get"
    path = "/health"
    protocol = "http"
    timeout = "2s"
    tls_skip_verify = false
    [services.http_checks.headers]

# Persistent volumes for logs and configuration
[[mounts]]
  source = "llm_logs"
  destination = "/app/logs"
  initial_size = "1gb"

[[mounts]]
  source = "llm_config"
  destination = "/app/config"
  initial_size = "100mb"

# Optional: Scale configuration
# [deploy]
#   # Auto stop/start machines to save costs
#   strategy = "rolling"
#
# [[vm]]
#   size = "shared-cpu-1x"
#   memory = "256mb"
